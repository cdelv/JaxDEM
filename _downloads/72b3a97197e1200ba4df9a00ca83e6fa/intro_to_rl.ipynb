{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Intro to JaxDEM Reinforcement Learning\n\nIn this example, we'll train a simple agent using JaxDEM's reinforcement learning tools.\n\nThe agent is a humble sphere that moves inside a box with reflective boundaries; the objective is\nto reach a target location. We train it with Proximal Policy Optimization (PPO)\n(:py:class:`~jaxdem.rl.trainers.PPOTrainer`) and a shared-parameters actor\u2013critic MLP\n(:py:class:`~jaxdem.rl.models.SharedActorCritic`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import distrax\nimport jax\nimport jax.numpy as jnp\n\nimport jaxdem as jdem\nimport jaxdem.rl as rl\n\nfrom flax import nnx\nimport optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment\nFirst, we create a single-agent navigation environment with reflective boundaries\n(uses sensible defaults for domain/time step internally). Check :py:class:`~jaxdem.rl.environments.SingleNavigator`\nfor details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = rl.Environment.create(\"singleNavigator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\nNext, we build a shared-parameters actor\u2013critic MLP. We can use a bijector to constrain the action space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "key = jax.random.key(1)\nkey, subkey = jax.random.split(key)\nmodel = rl.Model.create(\n    \"SharedActorCritic\",\n    key=nnx.Rngs(subkey),\n    observation_space_size=env.observation_space_size,\n    action_space_size=env.action_space_size,\n    action_space=rl.ActionSpace.create(\"maxNorm\", max_norm=6.0),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer (PPO)\nThen, we construct the PPO trainer; feel free to tweak learning rate, num_epochs, etc. (:py:class:`~jaxdem.rl.trainers.PPOTrainer`)\nThese parameters are chosen for the training to run very fast. Not really for quality. Using a bijector, we don't need to clip actions.\nHowever, if we wanted to, we could pass that option to the trainer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "key, subkey = jax.random.split(key)\ntr = rl.Trainer.create(\n    \"PPO\",\n    env=env,\n    model=model,\n    key=subkey,\n    num_epochs=100,  # 300\n    num_envs=512,\n    num_steps_epoch=32,\n    num_minibatches=4,\n    learning_rate=1e-1,  # 1e-1\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\nTrain the policy. Returns the updated trainer with learned parameters. This method is just a convenience\ntraining loop. If desired, one can iterate manually :py:meth:`~jaxdem.rl.trainers.trainer.epoch`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tr = tr.train(tr, directory=\"/tmp/runs\", verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the New Policy\nNow that we have a trained agent, let's play around with it.\n\nWe spawn the agent and periodically change the target it needs to go to. This way,\nwe will have the agent chasing around the objective. When saving the simulation state,\nwe add a small sphere to visualize where the agent needs to go.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tr.key, subkey = jax.random.split(tr.key)\ntr.env = env.reset(env, subkey)  # replace the vectorized env with the serial one\n\nwriter = jdem.VTKWriter(directory=\"/tmp/frames\")\nstate = tr.env.state.add(\n    tr.env.state, pos=tr.env.env_params[\"objective\"], rad=tr.env.state.rad / 5\n)\nstate.ID = state.ID.at[..., state.N // 2 :].set(state.ID[..., : state.N // 2])\n\nwriter.save(state, tr.env.system)\n\nfor i in range(1, 2000):\n    tr, _ = tr.step(tr)\n\n    if i % 10 == 0:\n        state = tr.env.state.add(\n            tr.env.state,\n            pos=tr.env.env_params[\"objective\"],\n            rad=tr.env.state.rad / 5,\n        )\n        state.ID = state.ID.at[..., state.N // 2 :].set(state.ID[..., : state.N // 2])\n\n        writer.save(state, tr.env.system)\n\n    # Change the objective without moving the agent\n    if i % 500 == 0:\n        key, subkey = jax.random.split(key)\n        min_pos = tr.env.state.rad[0] * jnp.ones_like(tr.env.system.domain.box_size)\n        objective = jax.random.uniform(\n            subkey,\n            (tr.env.max_num_agents, tr.env.state.dim),\n            minval=min_pos,\n            maxval=tr.env.system.domain.box_size - min_pos,\n            dtype=float,\n        )\n        tr.env.env_params[\"objective\"] = objective"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}